---
layout: post
title:  "[CICD Study 1기 by Gasida] - Speeding Up Container Image Builds in Tekton Pipelines"
categories: [CICD,helm,tekton]
tags: [container,helm,tekton]
---

# 들어가기 앞서  Tekton 소개

텍톤은 쿠버네티스 기반 오픈소스 클라우드 네이티브 CI/CD시스템이다. 쿠버네티스 클러스터에 확장 모듈로 설치 및 실행되며
CI/CD 파이프라인 구축에 쓰이는 CRD도 제공한다. 텍톤 엔진은 쿠버네티스 클러스터 내부에 상주하며 API 객체를 통해 수행할 작업을 선언적으로 정의할 수 있도록 한다.


## 전통적인 CI/CD
- 가상머신(VM) 기반으로 설계
- CI엔진 유지관리를 위해 IT 운영팀 필요
- CI엔진 전체에서 플러그인 공유
- 플러그인 의존성의 업데이트 주기가 불명확
- Kubernetes 리소스와 상호 운용성 없음
- 관리자가 직접 지속성(Persistence) 관리
- CI 엔진 컨테이너에 설정이 내장됨

## 클라우드 네이티브 CI/CD
- 컨테이너와 쿠버네티스를 위해 설계
- 운영 오버헤드 없는 서비스형 파이프라인
- 파이프라인이 서로 완전히 격리됨
- 모든 것이 컨테이너 이미지로 수명주기 관리
- 네이티브 쿠버네티스 리소스 활용
- 플랫폼이 지속성 관리
- 쿠버네티스 `ConfigMaps`을 통해 구성

> 핵심 차이점은 클라우드 네이티브 CI/CD는 Kubernetes 환경에 최적화되어 있으며, 격리성과 자동화가 뛰어나고 운영 부담이 적다. 반면 전통적인 방식은 중앙집중식 CI 서버에 의존하며 더 많은 수동 관리가 필요하다.


## 용어 정리

- `STEP` : Tekton의 가장 작은 단위, 파드내의 개별 컨테이너에 대응한다
- `TASK` : 파드에 대응, steps 목록을 가질수 있으며 재사용이 가능하다.
- `PIPELINE` : 앱을 빌드 및 또는 배포하는 데 필요한 Task의 목록. 파이프라인, TASK 들을 순차 / 병렬로 정의 task들간의 데이터 공유 설정 가능
- `TaskRun` : Task 인스턴스의 실행 및 그 결과
- `PipelineRun` : Pipeline 인스턴스의 실행 및 그 결과. 다수의 TaskRun 포함
- `Trigger` : 이벤트를 감지하고 다른 CRD에 연결하여 해당 이벤트가 발생했을 때 어떤 일이 발생하는지 지정.

# 기본 환경 구축

## kind를 이용한 구버네티스 배포

```bash
kind create cluster --name myk8s --image kindest/node:v1.32.8 --config - <<EOF
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  extraPortMappings:
  - containerPort: 30000
    hostPort: 30000  
EOF
```
## 텍톤 구성요소 최신 버전 설치 : Pipeline, Trigger, Dashboard

### Pipleline
```bash
# Tekton dependency 파이프라인(pipeline) 설치 : 현재 v1.5.0
kubectl apply -f https://storage.googleapis.com/tekton-releases/pipeline/latest/release.yaml

# Tekton dependency 파이프라인(pipeline) 설치 확인
kubectl get crd
kubectl get ns | grep tekton
kubectl get-all -n tekton-pipelines # kubectl krew install get-all
kubectl get all -n tekton-pipelines
kubectl get-all -n tekton-pipelines-resolvers
kubectl get all -n tekton-pipelines-resolvers

# 파드 확인
kubectl get pod -n tekton-pipelines
```

![img_10.png](/assets/cicd-2week-1/cicd-2week1-11.png)

### Trigger
```bash
# Tekton Trigger 설치 : 현재 v0.33.0
kubectl apply -f https://storage.googleapis.com/tekton-releases/triggers/latest/release.yaml
kubectl apply -f https://storage.googleapis.com/tekton-releases/triggers/latest/interceptors.yaml

# Tekton Trigger 설치 확인
kubectl get crd | grep triggers # trigger 관련 crd 확인
kubectl get all -n tekton-pipelines

# trigger 관련 deployment 설치 확인
k get deploy -n tekton-pipelines | grep triggers
```

### Dashboard

```bash
# Tekton Dashboard 설치 : 현재 v0.62.0
kubectl apply -f https://storage.googleapis.com/tekton-releases/dashboard/latest/release.yaml

# Tekton Dashboard 설치 확인
kubectl get crd | grep dashboard # dashboard 관련 crd 확인
kubectl get all -n tekton-pipelines

# Dashboard 관련 deployment 설치 확인
kubectl get deploy -n tekton-pipelines
tekton-dashboard                    1/1     1            1           81s
...

# service 정보 확인
kubectl get svc,ep -n tekton-pipelines tekton-dashboard
kubectl get svc -n tekton-pipelines tekton-dashboard -o yaml | kubectl neat | yq

# service 를 Nodeport 설정 : nodePort 30000
kubectl patch svc -n tekton-pipelines tekton-dashboard -p '{"spec":{"type":"NodePort","ports":[{"port":9097,"targetPort":9097,"nodePort":30000}]}}'
kubectl get svc,ep -n tekton-pipelines tekton-dashboard
```

대시보드 접속 확인
```bash
open http://localhost:30000 # macOS
웹 브라우저 http://<Ubuntu IP>:30000   # Windows WSL2 Ubuntu 경우
```

![img_11.png](/assets/cicd-2week-1/cicd-2week1-12.png)

## tekton CLI설치
```bash
# macOS
brew install tektoncd-cli

tkn version

# Windows WSL - Ubuntu Linux 경우
sudo apt update;sudo apt install -y gnupg
sudo mkdir -p /etc/apt/keyrings/
sudo gpg --no-default-keyring --keyring /etc/apt/keyrings/tektoncd.gpg --keyserver keyserver.ubuntu.com --recv-keys 3EFE0E0A2F2F60AA
echo "deb [signed-by=/etc/apt/keyrings/tektoncd.gpg] http://ppa.launchpad.net/tektoncd/cli/ubuntu eoan main"|sudo tee /etc/apt/sources.list.d/tektoncd-ubuntu-cli.list
sudo apt update && sudo apt install -y tektoncd-cli
```

버전확인 

![img_12.png](/assets/cicd-2week-1/cicd-2week1-13.png)

# Kaniko를 사용하여 Tekton 파이프라인에서 컨테이너 이미지 빌드속도 향상 시키기

> 원글참조 https://tekton.dev/blog/2023/11/02/speeding-up-container-image-builds-in-tekton-pipelines/

 
Kaniko 는 Dockerfile에서 컨테이너 이미지를 빌드하는 데 사용할 수 있는 오픈 소스 도구이며 Docker 데몬을 실행할 필요가 없으므로 Kubernetes 클러스터와 같이 Docker 데몬을 사용할 수 없거나 실행할 수 없는 환경에서 이미지를 빌드하는 데 이상적이다.

- 준비 사항
  - 위에서 준비한 Tekton Pipeline이 설치된 쿠버네티스 클러스터
  - Git 기반 소스 코드 관리 (SCM)
  - 컨테이너 레지스트리 (아티팩트 Push 권한 필요) - 도커이미지 저장 및 캐시를 저장하기 위한 레지스트리


## 자신의 Github에 Private Repo생성 후 코드  Push
> 샘플 앱코드 포크하기 (Optional)
> 
> 실습에 사용된 앱 레포지토리지만 이번 실습을 위해서는 `Dockerfile`만 있으면 되므로 크게 중요하진않다

```bash
https://github.com/google/docsy-example
```
![img_14.png](/assets/cicd-2week-1/cicd-2week1-15.png)



아래의 도커파일 작성하여 푸시하기

```dockerfile
FROM floryn90/hugo:ext-alpine

USER root 
RUN apk add git && \
  git config --global --add safe.directory /src
```
- Github에 푸시
```bash
git push -u origin main
```

## 컨테이너 레지스트리 이미지 저장소 인증정보 세팅

```bash
echo "https://index.docker.io/v1/" | docker-credential-osxkeychain get | jq
{
  "ServerURL": "https://index.docker.io/v1/",
  "Username": "<username>", -> 요값과 
  "Secret": "****"      -> 요값을 가지고 auth 생성
}

echo -n "<username>:****" | base64
AXDFGHXXCFGFGF

# ~/.docker/config.json 대신 임시 파일 dsh.txt 작성
vi dsh.txt
{
  "auths": {
    "https://index.docker.io/v1/": {
      "auth": "AXDFGHXXCFGFGF"
    }
  }
}
# dsh.txt 파일 내용을 다시 base64 적용
DSH=$(cat dsh.txt | base64 -w0)
echo $DSH
```

## Secret 세팅 및 ServiceAccount 연결
위에서 생성한 DSH값으로 Secret을 사용한다. 이는 나중에 파이프라인이 동작할때 도커레지스트리에 푸시하는 크리덴셜로 사용된다. 

```bash
cat << EOF | kubectl apply -f -
apiVersion: v1
kind: Secret
metadata:
  name: docker-credentials
data:
  config.json: $DSH
EOF
```


```bash
kubectl create sa build-sa
kubectl patch sa build-sa -p '{"secrets": [{"name": "docker-credentials"}]}'
kubectl get sa build-sa -o yaml | kubectl neat | yq
```

![img_15.png](/assets/cicd-2week-1/cicd-2week1-16.png)

## Task 설치

```bash
tkn hub install task kaniko
kubectl get tasks
kubectl get tasks kaniko -o yaml | k neat | yq
```

아래처럼 kaniko 이미지 및 도커 빌드시 아규먼트 확인. 다음에 다시 언급하겠지만 `args`부분
에 cache플래그는 보이지않는다.

![img_17.png](/assets/cicd-2week-1/cicd-2week1-18.png)


## Pipeline 설치

```bash
cat << EOF | kubectl apply -f -
apiVersion: tekton.dev/v1
kind: Pipeline
metadata:
  name: clone-build-push
spec:
  description: | 
    This pipeline clones a git repo, builds a Docker image with Kaniko and pushes it to a registry
  params:
  - name: repo-url
    type: string
  - name: image-reference
    type: string
  workspaces:
  - name: shared-data
  - name: docker-credentials
  tasks:
  - name: fetch-source
    taskRef:
      name: git-clone
    workspaces:
    - name: output
      workspace: shared-data
    params:
    - name: url
      value: \$(params.repo-url)
  - name: build-push
    runAfter: ["fetch-source"]
    taskRef:
      name: kaniko
    workspaces:
    - name: source
      workspace: shared-data
    - name: dockerconfig
      workspace: docker-credentials
    params:
    - name: IMAGE
      value: \$(params.image-reference)
EOF
```
Pipeline에는 지정한 Task들을 확인할 수 있다.
- fetch-source : git-clone태스크 참조하여 실행
- build-push : kaniko 태스크를 `fetch-source`다음에 실행

![img_19.png](/assets/cicd-2week-1/cicd-2week1-20.png)


## 파이프라인 실행
작성한 파이프라인을 실행하기 위해 `PipelineRun` 리소스를 생성한다. 

도커 레지스트리에 접근권한을 얻기 위해서 위에서 작성한 ServiceAccount를 추한다.

바로 위에서 작성한 `clone-build-push`라는 이름의 pipeline을 참조하고 있다. 

마지막으로 parameter로 가져올 git url과 도커 저장소의 url을 입력해준다. 
```bash 
cat << EOF | kubectl create -f -
apiVersion: tekton.dev/v1
kind: PipelineRun
metadata:
  generateName: clone-build-push-run-
spec:
  pipelineRef:
    name: clone-build-push
  taskRunTemplate:
    serviceAccountName: build-sa
    podTemplate:
      securityContext:
        fsGroup: 65532
  workspaces:
  - name: shared-data
    volumeClaimTemplate:
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 1Gi
  - name: docker-credentials
    secret:
      secretName: docker-credentials
  params:
  - name: repo-url
    value: https://github.com/hyeonjae1122/docsy-example.git
  - name: image-reference
    value: docker.io/hyeonjae0913/petclinic:1.0.0    # 각자 자신의 저장소
EOF
```

![img_20.png](/assets/cicd-2week-1/cicd-2week1-21.png)

아래의 명령어로 로그도 확인할 수 있다. 

```bash
tkn pipelinerun logs -f clone-build-push-cache-run-8rg7d
```
![img_21.png](/assets/cicd-2week-1/cicd-2week1-22.png)


## 캐시 옵션적용하기

- 이미 설치된 kaniko Task에 외부인자를 주어 cache 플래그를 활성화 시킨다. (이전 Pipeline과 구분하기 위해 name도 `clone-build-push-cache`로 변경)

```bash
cat << 'EOF' | kubectl apply -f -
apiVersion: tekton.dev/v1
kind: Pipeline
metadata:
  name: clone-build-push-cache
spec:
  description: | 
    This pipeline clones a git repo, builds a Docker image with Kaniko and pushes it to a registry
  params:
  - name: repo-url
    type: string
  - name: image-reference
    type: string
  workspaces:
  - name: shared-data
  - name: docker-credentials
  tasks:
  - name: fetch-source
    taskRef:
      name: git-clone
    workspaces:
    - name: output
      workspace: shared-data
    params:
    - name: url
      value: $(params.repo-url)
  - name: build-push
    runAfter: ["fetch-source"]
    taskRef:
      name: kaniko
    workspaces:
    - name: source
      workspace: shared-data
    - name: dockerconfig
      workspace: docker-credentials
    params:
    - name: IMAGE
      value: $(params.image-reference)
    - name: EXTRA_ARGS
      value:
        - "--cache=true"
        - "--cache-ttl=168h"
        - "--cache-repo=docker.io/hyeonjae0913/petclinic"        
EOF
```
- 새롭게 추가된 부분 

![img_23.png](/assets/cicd-2week-1/cicd-2week1-24.png)

- 아래 명령어 새롭게 추가된 Pipeline(`clone-build-push-cache`)를 실행한다. 
```bash
cat << EOF | kubectl create -f -
apiVersion: tekton.dev/v1
kind: PipelineRun
metadata:
  generateName: clone-build-push-run-
spec:
  pipelineRef:
    name: clone-build-push-cache
  taskRunTemplate:
    serviceAccountName: build-sa
    podTemplate:
      securityContext:
        fsGroup: 65532
  workspaces:
  - name: shared-data
    volumeClaimTemplate:
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 1Gi
  - name: docker-credentials
    secret:
      secretName: docker-credentials
  params:
  - name: repo-url
    value: https://github.com/hyeonjae1122/docsy-example.git
  - name: image-reference
    value: docker.io/hyeonjae0913/petclinic:1.0.0    # 각자 자신의 저장소
EOF
```

![img_24.png](/assets/cicd-2week-1/cicd-2week1-25.png)

위와 마찬가지로 cache가 적용된 Pipeline이 동작한다. 

## 빌드 로그 확인

이제 위에서 cache 적용 / 미적용 케이스의 Pipeline을 실행하였으므로 대시보드에서 로그를 확인한다. 

먼저 cache를 적용하지 않았을때의 로그 모습이다. 

![img_6.png](/assets/cicd-2week-1/cicd-2week1-7.png)

다음은 cache를 적용하였을때의 로그 모습이다. 차이점은 중간에 빨간 박스에 있는 것처럼 위에서 지정한
레지스트리에서 캐싱된 레이어가 존재하는지 체크하여 존재한다면 사용함으로써 빌드시간을 단축시킨다. 

![img_7.png](/assets/cicd-2week-1/cicd-2week1-8.png)

- 아래와 같이 실제로 도커 저장소에서 확인해보면 캐쉬된 이미지의 레이어를 확인할 수 있다. 
어떤 이미지 레이어가 캐시되어있는지, 어떤명령어에 해당되는지도 알 수 있다. 

![img_9.png](/assets/cicd-2week-1/cicd-2week1-10.png)

## 실제 빌드 속도 체감

아래 화면의 빨간색 네모박스가 cache를 활성화 했을 경우, 그리고 하늘색 네모박스가 캐쉬를 사용하지 않았을 경우이다.

- Cache Enable 의 경우 최대 20~22초
- Cache Disable 의 경우 최대 28초

물론 레이어 하나의 부분만 캐쉬되어있으며 해당 프로젝트의 빌드 시간차이가 크게 느껴지지 않을 수도 있지만
개발자 입장에서 빌드시간이 개발 효율성을 크게 차지하고 있는 지금, 공통화 된 레이어를 캐쉬화 하는 작업은
굉장히 중요한 부분일 수 있다. 

![img_8.png](/assets/cicd-2week-1/cicd-2week1-9.png)


## pv에 캐시하기 (실제 캐싱 테스트까진 검증 실패)

- Task 에 Volume과 VolumeMount추가

```bash
kubectl patch task kaniko --type='json' -p='[
  {
    "op": "add",
    "path": "/spec/steps/0/volumeMounts",
    "value": [
      {
        "name": "basecache",
        "mountPath": "/cache"
      }
    ]
  },
  {
    "op": "add",
    "path": "/spec/volumes",
    "value": [
      {
        "name": "basecache",
        "persistentVolumeClaim": {
          "claimName": "basecache-pvc"
        }
      }
    ]
  }
]'
```

- basecache PVC 생성


```bash
cat << EOF | kubectl apply -f -
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: basecache-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
EOF
```

## kaniko-warmer 파드로 베이스 이미지 미리 다운로드

-  베이스 이미지를 다운받는 `kaniko-warmer` 파드를 생성한다. 각 Dockerfile에 필요한 베이스 이미지가 기술 되어 있을 것이다.
   아래의 경우 `node:20-alpine` 이미지와 `eclipse-temurin:21-jre-jammy` 등 필요한 이미지를 PV에 미리 저장해둔다.

```bash
cat << EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: kaniko-warmer
spec:
  restartPolicy: Never
  containers:
  - name: warmer
    image: gcr.io/kaniko-project/warmer:latest
    args:
      - --cache-dir=/cache
      - --image=node:20-alpine
      - --image=eclipse-temurin:21-jre-jammy
      - --image=floryn90/hugo:ext-alpine 
    volumeMounts:
      - name: basecache
        mountPath: /cache
  volumes:
    - name: basecache
      persistentVolumeClaim:
        claimName: basecache-pvc
EOF
```

- 파드 로그 확인

![img_25.png](/assets/cicd-2week-1/cicd-2week1-26.png)

<br><br>
- 임시 Pod로 PVC 마운트해서 캐시파일 확인하기

```bash
cat << EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: cache-inspector
spec:
  containers:
  - name: inspector
    image: busybox
    command: ['sh', '-c', 'sleep 3600']
    volumeMounts:
    - name: basecache
      mountPath: /cache
  volumes:
  - name: basecache
    persistentVolumeClaim:
      claimName: basecache-pvc
EOF
```


```bash
# Pod가 Running 상태가 될 때까지 대기
kubectl wait --for=condition=Ready pod/cache-inspector --timeout=60s

# /cache 디렉토리 확인
kubectl exec -it cache-inspector -- ls -lah /cache
```
<br><br>
실제로 캐시 저장된 것을 확인

```
/cache/sha256:4104c...
```

![img_26.png](/assets/cicd-2week-1/cicd-2week1-27.png)

<br><br>

- 파이프라인 생성
```bash
cat << 'EOF' | kubectl apply -f -
apiVersion: tekton.dev/v1
kind: Pipeline
metadata:
  name: clone-image-build
spec:
  description: | 
    This pipeline clones a git repo, builds a Docker image with Kaniko and pushes it to a registry
  params:
  - name: repo-url
    type: string
  - name: image-reference
    type: string
  workspaces:
  - name: shared-data
  - name: docker-credentials
  tasks:
  - name: fetch-source
    taskRef:
      name: git-clone
    workspaces:
    - name: output
      workspace: shared-data
    params:
    - name: url
      value: $(params.repo-url)
  - name: build-push
    runAfter: ["fetch-source"]
    taskRef:
      name: image-build
    workspaces:
    - name: source
      workspace: shared-data
    - name: dockerconfig
      workspace: docker-credentials
    params:
    - name: IMAGE
      value: $(params.image-reference)    
EOF
```


<br><br>

- PipelineRun 실행

```bash
cat << EOF | kubectl create -f -
apiVersion: tekton.dev/v1
kind: PipelineRun
metadata:
  generateName: clone-build-push-run-
spec:
  pipelineRef:
    name: clone-build-push
  taskRunTemplate:
    serviceAccountName: build-sa
    podTemplate:
      securityContext:
        fsGroup: 65532
  workspaces:
  - name: shared-data
    volumeClaimTemplate:
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 1Gi
  - name: docker-credentials
    secret:
      secretName: docker-credentials
  params:
  - name: repo-url
    value: https://github.com/hyeonjae1122/docsy-example.git
  - name: image-reference
    value: docker.io/hyeonjae0913/petclinic:1.0.0    # 각자 자신의 저장소
EOF
```


문제점은 자꾸 원격으로 캐싱을 하려한다는 점인데 요점은 캐시파일을 쿠버네티스 리소스 PV에 저장해둔다는 점이다. 
- `--cache-dir`로 캐시파일이 저장 될 마운트 경로를 지정

아래는 공식 블로그 사진

- 캐싱된 경로가 /cache~ 로시작하여 로컬의 Persistant Disk에서 가져온것을 확인할 수 있다.  
![img_27.png](/assets/cicd-2week-1/cicd-2week1-28.png)
