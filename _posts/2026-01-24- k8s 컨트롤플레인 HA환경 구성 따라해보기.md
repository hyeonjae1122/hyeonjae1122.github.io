---
layout: post
title: "[K8S Deploy Study by Gasida] - k8s 컨트롤플레인 HA환경 구성 따라해보기"
categories:
  - "\bAnsible"
tags:
  - k8s
  - kubeadm
---
# 실습 환경 배포

```bash
# 실습용 디렉터리 생성
mkdir k8s-ha-upgrade
cd k8s-ha-upgrade

curl -O https://raw.githubusercontent.com/gasida/vagrant-lab/refs/heads/main/k8s-ha-upgrade/Vagrantfile
curl -O https://raw.githubusercontent.com/gasida/vagrant-lab/refs/heads/main/k8s-ha-upgrade/admin-lb.sh
curl -O https://raw.githubusercontent.com/gasida/vagrant-lab/refs/heads/main/k8s-ha-upgrade/init_cfg.sh
curl -O https://raw.githubusercontent.com/gasida/vagrant-lab/refs/heads/main/k8s-ha-upgrade/k8s-ctr.sh

# 실습 환경 배포
vagrant up
vagrant status
```

# kubeam에 의한 배포 과정 

## 1. HAProxy 동작 확인

로드 밸런서 진입

```
vagrant ssh admin-lb
```


```
cat /etc/haproxy/haproxy.cfg
```

6443 포트가 바인딩 되어있으며 k8s-api 호출 (TCP 6443 호출)시 3개의 컨트롤 플레인에 분산 되었는지 설정을 확인한다.


![[2026-01-24- k8s 컨트롤플레인 HA환경 구성 따라해보기-1.png]]

상태확인 커맨드 

```bash
systemctl status haproxy.service --no-pager
journalctl -u haproxy.service --no-pager
ss -tnlp | grep haproxy
```

![[2026-01-24- k8s 컨트롤플레인 HA환경 구성 따라해보기-2.png]]


아래 명령어로 외부(실습과정에선 MacOS에서)에서 접속 해보자.


```bash
open http://192.168.10.10:9000/haproxy_stats
```

backend 대상은 아직 DOWN상태

![[2026-01-24- k8s 컨트롤플레인 HA환경 구성 따라해보기-3.png]]

## 2. 3개의 컨트롤 플레인 설정

첫번째 과정에서 로드밸런서 설정 및 동작을 확인하였다. 이제 컨트롤 플레인 노드를 Init하고 2개의 나머지 컨트롤 플레인 노드를 Join시켜 컨트롤플레인 클러스터를 작동시켜보자. 

### 2.1 첫번째 컨트롤 플레인 설정

- 모든 컨트롤 플레인에서 공유하는 CA인증서(10년짜리)를 생성한다
    - kubernetes용 ca.crt / ca.key
    - etcd용 etcd-ca.crt / etcd-ca.key
    - front-proxy 용 (front-proxy-ca.crt / front-proxy-ca.key)
- 해당 인증서를 다른 컨트롤 플레인과 공유하도록 Secret에 인증서/키 저장한다
- CA 인증서를 기반으로 k8s-ctr1 내에서 동작하는 Static Pod등 필요한 인증서 생성
    - kube-apiserver의 TLS Web Server 인증서 등


#### 2.1.1 kubeadm init 실행

```bash
kubeadm init --control-plane-endpoint "192.168.10.10:6443" --upload-certs --apiserver-advertise-address 192.168.10.11 --pod-network-cidr=10.244.0.0/16 --service-cidr=10.96.0.0/16

```


아래 메모 해두기 : 나머지 컨트플 플레인 노드가 join 시

```
kubeadm join 192.168.10.10:6443 --token avsvhy.gfsu1lllclhpekbt \
	--discovery-token-ca-cert-hash sha256:b1259ad74b71d625d37303181583f0bdd4ff5f6b2db615382abddb5670c48394 \
	--control-plane --certificate-key 7bc68174c417cee8baa4526993ea080efa707c4ffdcc8252abe916e337b2dc99
```

아래 메모 해두기 : 워커 노드가 join 시

```
kubeadm join 192.168.10.10:6443 --token avsvhy.gfsu1lllclhpekbt \
	--discovery-token-ca-cert-hash sha256:b1259ad74b71d625d37303181583f0bdd4ff5f6b2db615382abddb5670c48394
```

#### 2.1.2 admin 자격증명 설정

```bash
mkdir -p $HOME/.kube
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) $HOME/.kube/config
kubectl config rename-context "kubernetes-admin@kubernetes" "HomeLab"
kubens default
```

#### 2.1.3 k8s 정보 확인

```
kubectl cluster-info
kubectl get node -owide
kubectl get secret -n kube-system kubeadm-certs
kubectl get secret -n kube-system kubeadm-certs -o yaml
kubectl get cm -n kube-system kubeadm-config -o yaml | grep certificatesDir
tree /etc/kubernetes/pki/
```

etcdctl 환경변수 설정

```
# etcdctl 환경변수 설정 export ETCDCTL_API=3 # etcd v3 API를 사용 export ETCDCTL_CACERT=/etc/kubernetes/pki/etcd/ca.crt export ETCDCTL_CERT=/etc/kubernetes/pki/etcd/server.crt export ETCDCTL_KEY=/etc/kubernetes/pki/etcd/server.key export ETCDCTL_ENDPOINTS=https://127.0.0.1:2379 # listen-client-urls

etcdctl member list -w table
etcdctl endpoint status -w table
```



![[2026-01-24- k8s 컨트롤플레인 HA환경 구성 따라해보기-4.png]]




```
# csr 확인
kubectl get csr csr-85w8j -o yaml
kubectl get csr
```

### 2.2 나머지 컨트롤 플레인 설정

- k8s-ctr1으로 부터 CA 인증서(10년)를 다운로드해서 자신의 노드로 가져옴
- CA 인증서를 기반으로 k8s-ctr2, k8s-ctr3 내에서 동작하는 Static Pod등 필요한 인증서 생성
    - - kube-apiserver의 TLS Web Server 인증서 등

## 3. k8s api 호출 확인 및 admin용 config설정

## 4. 워커노드 조인
## 5. 첫번째 컨트롤 플러그인에 각종 플러그인 설치

## 
