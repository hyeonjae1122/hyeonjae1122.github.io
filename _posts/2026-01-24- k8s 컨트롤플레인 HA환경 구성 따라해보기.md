---
layout: post
title: "[K8S Deploy Study by Gasida] - k8s 컨트롤플레인 HA환경 구성 따라해보기"
categories:
  - "\bAnsible"
tags:
  - k8s
  - kubeadm
---
# 실습 환경 배포

- 구축하고자하는 환경

![](https://raw.githubusercontent.com/hyeonjae1122/hyeonjae1122.github.io/main/assets/20260124T080514680Z.png)

| NAME         | Description        | CPU | RAM | NIC1      | NIC2               | Init Script              |
| ------------ | ------------------ | --- | --- | --------- | ------------------ | ------------------------ |
| **admin-lb** | k8s-api LB, 호출 테스트 | 2   | 1GB | 10.0.2.15 | **192.168.10.10**  | admin-lb.sh              |
| **k8s-ctr1** | K8S ControlPlane   | 4   | 2GB | 10.0.2.15 | **192.168.10.11**  | init-cfg.sh → k8s-ctr.sh |
| **k8s-ctr2** | K8S ControlPlane   | 4   | 2GB | 10.0.2.15 | **192.168.10.12**  | init-cfg.sh → k8s-ctr.sh |
| **k8s-ctr3** | K8S ControlPlane   | 4   | 2GB | 10.0.2.15 | **192.168.10.13**  | init-cfg.sh → k8s-ctr.sh |
| **k8s-w1**   | K8S Worker         | 2   | 2GB | 10.0.2.15 | **192.168.10.101** | init-cfg.sh              |
| **k8s-w2**   | K8S Worker         | 2   | 2GB | 10.0.2.15 | **192.168.10.102** | init-cfg.sh              |

**주요 설치 항목 버전**

| 설치 항목                                                   | 버전       |
| ------------------------------------------------------- | -------- |
| kube-apiserver, kube-controller-manager, kube-scheduler | v1.32.11 |
| kubelet                                                 | v1.32.11 |
| kube-proxy                                              | v1.32.11 |
| containerd                                              | v2.1.5   |
| etcd                                                    | 3.5.24   |
| coredns                                                 | v1.11.3  |
| flannel cni plugin                                      | v0.27.3  |
| prometheus                                              | 3.9.1    |
| grafana                                                 | 12.3.1   |

실습 환경 배포를 위한 디렉터리 생성 및 스크립트 다운로드

```bash
mkdir k8s-ha-upgrade
cd k8s-ha-upgrade

curl -O https://raw.githubusercontent.com/gasida/vagrant-lab/refs/heads/main/k8s-ha-upgrade/Vagrantfile
curl -O https://raw.githubusercontent.com/gasida/vagrant-lab/refs/heads/main/k8s-ha-upgrade/admin-lb.sh
curl -O https://raw.githubusercontent.com/gasida/vagrant-lab/refs/heads/main/k8s-ha-upgrade/init_cfg.sh
curl -O https://raw.githubusercontent.com/gasida/vagrant-lab/refs/heads/main/k8s-ha-upgrade/k8s-ctr.sh
```

배포 

```bash
vagrant up
vagrant status
```

# kubeam에 의한 k8s 배포

## 1. HAProxy 동작 확인

로드 밸런서 진입

```
vagrant ssh admin-lb
```

haproxy config파일 확인

```
cat /etc/haproxy/haproxy.cfg
```

6443 포트가 바인딩 되어있으며 k8s-api 호출 (TCP 6443 호출)시 3개의 컨트롤 플레인에 분산 되었는지 설정을 확인한다.

![](https://raw.githubusercontent.com/hyeonjae1122/hyeonjae1122.github.io/main/assets/20260124T081935799Z.png)


상태 확인 커맨드 

```bash
systemctl status haproxy.service --no-pager
journalctl -u haproxy.service --no-pager
ss -tnlp | grep haproxy
```

![](https://raw.githubusercontent.com/hyeonjae1122/hyeonjae1122.github.io/main/assets/20260124T082013767Z.png)

![](https://raw.githubusercontent.com/hyeonjae1122/hyeonjae1122.github.io/main/assets/20260124T082023045Z.png)


아래 명령어로 외부(실습과정에선 MacOS에서)에서 접속 해보자.

```bash
open http://192.168.10.10:9000/haproxy_stats
```


backend 대상은 아직 DOWN상태도 확인하였다.

![](https://raw.githubusercontent.com/hyeonjae1122/hyeonjae1122.github.io/main/assets/20260124T082148348Z.png)

## 2. 3개의 컨트롤 플레인 설정

첫번째 과정에서 `HAProxy` 로드밸런서 설정 및 동작을 확인하였다. 이제 첫번째 컨트롤 플레인 노드를 `Init`하고 2개의 나머지 컨트롤 플레인 노드를 `Join`시켜 컨트롤플레인 클러스터를 구축시켜보자. 

### 2.1 첫번째 컨트롤 플레인 설정

- 모든 컨트롤 플레인에서 공유하는 CA인증서(10년짜리)를 생성한다
    - kubernetes용 `ca.crt` / `ca.key`
    - etcd용 `etcd-ca.crt` / `etcd-ca.key`
    - front-proxy 용 (`front-proxy-ca.crt` / `front-proxy-ca.key`)
- 해당 인증서를 다른 컨트롤 플레인과 공유하도록 `Secret`에 인증서/키 저장한다
- CA 인증서를 기반으로 k8s-ctr1 내에서 동작하는 Static Pod등 필요한 인증서 생성
    - kube-apiserver의 TLS Web Server 인증서 등


#### 2.1.1 kubeadm init 실행

```bash
kubeadm init --control-plane-endpoint "192.168.10.10:6443" --upload-certs --apiserver-advertise-address 192.168.10.11 --pod-network-cidr=10.244.0.0/16 --service-cidr=10.96.0.0/16
```


아래 메모 해두기 : 나머지 컨트플 플레인 노드가 `join` 시 필요

```bash
kubeadm join 192.168.10.10:6443 --token t31uuc.grqyncfd64urgl63 \
	--discovery-token-ca-cert-hash sha256:5137d78bddbdf2c4314490994d7f6dc8677e88b82c581ad27408c0e784a34197 \
	--control-plane --certificate-key 9d1c1084d45390585a9ba92d39d16684f90fd1ae0281d8fb82bb3162fe69aff9
```

아래 메모 해두기 : 워커 노드가 join 시

```bash
kubeadm join 192.168.10.10:6443 --token t31uuc.grqyncfd64urgl63 \
	--discovery-token-ca-cert-hash sha256:5137d78bddbdf2c4314490994d7f6dc8677e88b82c581ad27408c0e784a34197
```

#### 2.1.2 admin 자격증명 설정

```bash
mkdir -p $HOME/.kube
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) $HOME/.kube/config
kubectl config rename-context "kubernetes-admin@kubernetes" "HomeLab"
kubens default
```

#### 2.1.3 k8s 정보 확인

클러스터 정보 및 노드 정보확인

```bash
kubectl cluster-info
kubectl get node -owide
```

kubeadm init에 `--upload-certs` 설정으로 인증서 정보가 시크릿으로 생성되었으며 이를 확인한다. 

```bash
kubectl get secret -n kube-system kubeadm-certs
kubectl get secret -n kube-system kubeadm-certs -o yaml
```

![[2026-01-24- k8s 컨트롤플레인 HA환경 구성 따라해보기-5.png]]

kubeadm-config configmap 에 인증서 디렉터리 확인

```

kubectl get cm -n kube-system kubeadm-config -o yaml | grep certificatesDir
```


인증서 디렉터리 확인

```
tree /etc/kubernetes/pki/
```


![[2026-01-24- k8s 컨트롤플레인 HA환경 구성 따라해보기-6.png]]


etcdctl 환경변수 설정 및 확인

```bash
# etcdctl 환경변수 설정 export ETCDCTL_API=3 # etcd v3 API를 사용 export ETCDCTL_CACERT=/etc/kubernetes/pki/etcd/ca.crt export ETCDCTL_CERT=/etc/kubernetes/pki/etcd/server.crt export ETCDCTL_KEY=/etc/kubernetes/pki/etcd/server.key export ETCDCTL_ENDPOINTS=https://127.0.0.1:2379 # listen-client-urls

etcdctl member list -w table
etcdctl endpoint status -w table
```

아직 etcd멤버는 k8s-ctr1 한대이며 리더역시 자신이 하고있음을 확인

![[2026-01-24- k8s 컨트롤플레인 HA환경 구성 따라해보기-4.png]]

csr 확인

```bash
kubectl get csr csr-85w8j -o yaml
kubectl get csr
```

![[2026-01-24- k8s 컨트롤플레인 HA환경 구성 따라해보기-7.png]]

### 2.2 나머지 컨트롤 플레인 설정

- `k8s-ctr1`**으로 부터 CA 인증서(10년)를 다운로드해서 자신의 노드로 가져옴**
- CA 인증서를 기반으로 k8s-ctr2, k8s-ctr3 내에서 동작하는 Static Pod등 필요한 인증서 생성
    - - kube-apiserver의 TLS Web Server 인증서 등


#### 2.2.1 나머지 컨트롤 플레인 join하기

위에서 메모한 토큰 및 키를 사용하여 컨트롤 플레인 join하자. 

- `--apiserver-advertise-address 192.168.10.12` 를 추가하는 것에 주의

```bash
kubeadm join 192.168.10.10:6443 --token t31uuc.grqyncfd64urgl63 \
	--discovery-token-ca-cert-hash sha256:5137d78bddbdf2c4314490994d7f6dc8677e88b82c581ad27408c0e784a34197 \
	--control-plane --certificate-key 9d1c1084d45390585a9ba92d39d16684f90fd1ae0281d8fb82bb3162fe69aff9 \
	--apiserver-advertise-address 192.168.10.12
```

두번째 컨트롤 플레인이 join을 시도할 경우 `kube-system` 네임스페이스의 `kubeadm-certs`의 시크릿으로부터 인증서를 다운로드하여 자신의 `/etc/kubernetes/pki` 경로에 저장하는 과정을 확인할 수 있다.

![[2026-01-24- k8s 컨트롤플레인 HA환경 구성 따라해보기-8.png]]

etcd cluster에 헬스체크를 진행한 후 새로운 멤버로 join하는 과정도 확인할 수 있다. 


![[2026-01-24- k8s 컨트롤플레인 HA환경 구성 따라해보기-9.png]]
#### 2.2.2  admin 자격증명 설정

```bash
mkdir -p $HOME/.kube
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) $HOME/.kube/config
kubectl config rename-context "kubernetes-admin@kubernetes" "HomeLab"
kubens default
```
#### 2.2.3 k8s 정보 확인

```
kubectl cluster-info
kubectl get node -owide
```

![[2026-01-24- k8s 컨트롤플레인 HA환경 구성 따라해보기-10.png]]

```
tree /etc/kubernetes/pki/
kubectl get secret -n kube-system kubeadm-certs -o yaml
kubectl get secret -n kube-system kubeadm-certs
```

![[2026-01-24- k8s 컨트롤플레인 HA환경 구성 따라해보기-12.png]]


![[2026-01-24- k8s 컨트롤플레인 HA환경 구성 따라해보기-11.png]]


```bash
# etcdctl 환경변수 설정
export ETCDCTL_API=3 # etcd v3 API를 사용
export ETCDCTL_CACERT=/etc/kubernetes/pki/etcd/ca.crt
export ETCDCTL_CERT=/etc/kubernetes/pki/etcd/server.crt
export ETCDCTL_KEY=/etc/kubernetes/pki/etcd/server.key
export ETCDCTL_ENDPOINTS=https://127.0.0.1:2379 # listen-client-urls

```

kubeadm join명령어로 `k8s-ctr3`도 마찬가지로 동일하게 진행해준다. 

```
kubeadm join 192.168.10.10:6443 --token t31uuc.grqyncfd64urgl63 \
	--discovery-token-ca-cert-hash sha256:5137d78bddbdf2c4314490994d7f6dc8677e88b82c581ad27408c0e784a34197 \
	--control-plane --certificate-key 9d1c1084d45390585a9ba92d39d16684f90fd1ae0281d8fb82bb3162fe69aff9 \
	--apiserver-advertise-address 192.168.10.13
```


최종 결과 확인
- 클러스터내의 총 3개의 컨트롤 플레인이 join 되었다. 

```
etcdctl member list -w table
etcdctl endpoint status -w table
```

![](https://raw.githubusercontent.com/hyeonjae1122/hyeonjae1122.github.io/main/assets/20260124T073105627Z.png)


‘HAProxy 통계 페이지’에 backend 대상 서버 3대 UP 확인.

![](https://raw.githubusercontent.com/hyeonjae1122/hyeonjae1122.github.io/main/assets/20260124T073132757Z.png)
## 3. k8s api 호출 확인 및 admin용 config설정

![](https://raw.githubusercontent.com/hyeonjae1122/hyeonjae1122.github.io/main/assets/20260124T073302519Z.png)

k8s api 호출 확인


`vagrant k8s admin-lb` 로 접근
- 각 컨트롤 플레인 IP에 대해서 접근 확인

```
```bash
curl -sk https://192.168.10.10:6443/api/v1/namespaces/kube-public/configmaps/cluster-info | jq
curl -sk https://192.168.10.10:6443/version | jq

curl -sk https://192.168.10.11:6443/version
curl -sk https://192.168.10.12:6443/version
curl -sk https://192.168.10.13:6443/version
```


## 3.1 admin용 kubeconfig 설정

로드밸런서에서 접근하기 위해 MacOS 터미널에서 `vagrant ssh k8s-ctr1` 접속하여 `admin.conf`파일을 복사하자. 

```bash
cat /etc/kubernetes/admin.conf 
```

`admin.conf` 파일 내용예시

```bash
# 일부생략
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURCVENDQWUyZ0F3SUJBZ0lJTkxkcEpzL21HSjh3RFFZSktvWklo....................MWx6eTBwNFI2SUljeXZYMkFrZW1qRUZrTEQ2ZFZHMWJTakFpL01tZjZ1NzVqKzZ3TXc0CjduUDVnRHUvOFNnd3l2Z0k4MkRXdVhZQ09RVDltUi8vbGR6WXkya3lCcXgvNWQvVUlMcm12S2U5a0xZK3FUMC8KNm4ySnZ0WHRyYjNzCi0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
    server: https://192.168.10.10:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kubernetes-admin
  name: kubernetes-admin@kubernetes
current-context: kubernetes-admin@kubernetes
kind: Config
preferences: {}
users:
- name: kubernetes-admin
  user:
    client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURLVENDQWhHZ0F3SUJBZ0lJZlMxTjlVWmJoMHd3RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TmpBeE1qUXdOekl4T............bnR4ZVdlN3RvemJCWWdrNmE5b05NZ01HcjR0TkpUU1IyVmZqN2RBYkNvYmhNVFcKaDhOR21RelNBNytiT1VJWkcrQjJZQ2V6SjJ1WHhXU2k2ZXcxS01nck1GSkh1T0ZYMnlXOS9DUyt0bGt6Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
    client-key-data: LS0tLS1C......2NITmpSeTBIYXVSbXBkTm84eVhSTGtMT25sRgpya1FHUW1YUGVsaFcvRlAraGgrc2J4a0grTlZRZW9mL1JmMVFIOGJwL2lwVG1GRmphZDhrMlE9PQotLS0tLUVORCBSU0EgUFJJVkFURSBLRVktLS0tLQo=
```


다시 `vagrant ssh admin-lb` 접속 `k8s-ctr1` 에서 복사한 내용을 아래 붙여넣는다

```bash
mkdir ~/.kube
touch ~/.kube/config
vi ~/.kube/config   
```

admin 용 `kubeconfig` 자격증명 확인

```bash
kubectl get node -owide
watch -d kubectl get node -owide
```

아직은 NotReady 상태이지만 3개의 컨트롤 플레인이 구성되었다.

![](https://raw.githubusercontent.com/hyeonjae1122/hyeonjae1122.github.io/main/assets/20260124T073546701Z.png)

## 4. 워커노드 조인

- k8s-w1 조인하기 
    - `kubeadm join` 실행
    - `Preflight Checks`(커널, 컨테이너 런타임, 메모리 등 환경점검)
    - `Cluster Discovery` (CA Hash를 이용해 LB에 접속, Cluster-info 및 CA 인증서를 획득한다)
    - TLS Bootstrap : 토큰을 사용해 임시 인증서 발급 후 API 서버에 인증요청
    - `Kubeconfig` 생성 : `/etc/kubernetes/kubelet.conf` 설정 파일 자동생성
    - `kubelet` 기동 : 서비스 시작 및 노드 상태 보고 시작

관리자 계정 전환해두기

```bash
echo "sudo su -" >> /home/vagrant/.bashrc
sudo su -
```

기본 정보 확인 하기 

```bash
ip -br -c -4 addr
cat /etc/sysconfig/kubelet
kubeadm version
```

워커노드로 조인

```bash
kubeadm join 192.168.10.10:6443 --token t31uuc.grqyncfd64urgl63 \
	--discovery-token-ca-cert-hash sha256:5137d78bddbdf2c4314490994d7f6dc8677e88b82c581ad27408c0e784a34197
```


`kubelet systemd` 확인

```bash
systemctl status kubelet --no-pager
```

`kubelet` 관련 파일 확인

```bash
tree /etc/kubernetes/
/etc/kubernetes/
├── kubelet.conf
├── manifests
└── pki
    └── ca.crt
```

`kubelet` 에서  `k8s api` 통신 시, k8s api 접속 엔드포인트 확인

```bash
cat /etc/kubernetes/kubelet.conf
cat /etc/kubernetes/kubelet.conf | grep server
    server: https://192.168.10.10:6443
```

kubelet 설정 관련 파일 확인

```bash
cat /var/lib/kubelet/config.yaml
cat /var/lib/kubelet/kubeadm-flags.env
cat /etc/sysconfig/kubelet
```


macOS에서 `vagrant ssh k8s-ctr` 

노드 상태확인

```bash
kubectl get node -owide
```

![](https://raw.githubusercontent.com/hyeonjae1122/hyeonjae1122.github.io/main/assets/20260124T074340212Z.png)

csr 확인

```bash
kubectl get csr
```

![](https://raw.githubusercontent.com/hyeonjae1122/hyeonjae1122.github.io/main/assets/20260124T074434008Z.png)


k8s-w2도 동일하게 진행한다. 

최종 결과 확인

![](https://raw.githubusercontent.com/hyeonjae1122/hyeonjae1122.github.io/main/assets/20260124T074608602Z.png)

## 5. 첫번째 컨트롤 플러그인에 각종 플러그인 설치

## 5.1 Flannel CNI 플러그인

- `Kube-controller-manager`는 각 노드에 파드 CIDR을 할당하는 역할을 한다. 각 파드는 파드 CIDR에서 고유한 IP 주소를 할당받는다.  
- `kubelet`은 컨테이너 런타임과 상호작용하여 예약된 Pod를 시작한다. 컨테이너 런타임의 일부인 CRI 플러그인은 CNI 플러그인과 상호작용하여 Pod 네트워크를 구성한다. 
- CNI 플러그인은 오버레이 네트워크를 사용하여 동일하거나 다른 노드에 분산된 파드간 네트워킹을 가능한
![](https://raw.githubusercontent.com/hyeonjae1122/hyeonjae1122.github.io/main/assets/20260124T093727505Z.png)

헬름 레포 추가 및 `kube-flannel` 네임스페이스 추가 

```bash
helm repo add flannel https://flannel-io.github.io/flannel
kubectl create namespace kube-flannel
```

flannel 설정

```bash
cat << EOF > flannel.yaml
podCidr: "10.244.0.0/16"
flannel:
  cniBinDir: "/opt/cni/bin"    # CNI 플러그인 바이너리 위치, kubelet이 CNI 실행 파일을 찾는 디렉터리
  cniConfDir: "/etc/cni/net.d" # CNI 네트워크 설정 파일 위치
  args:
  - "--ip-masq"                # SNAT (IP Masquerade) 활성화, Pod에서 외부 네트워크 트래픽 시 Source IP를 Node IP로 변환
  - "--kube-subnet-mgr"        # Kubernetes API 기반 subnet 관리, Node.spec.podCIDR 정보를 사용
  - "--iface=enp0s9"           # Flannel VXLAN 터널이 바인딩될 NIC 지정
  backend: "vxlan"             # Flannel 네트워크 캡슐화 방식
EOF
```

flannel 설치

```bash
helm install flannel flannel/flannel --namespace kube-flannel --version 0.27.3 -f flannel.yaml
```

확인

```bash
kubectl get node
```

![](https://raw.githubusercontent.com/hyeonjae1122/hyeonjae1122.github.io/main/assets/20260124T074753448Z.png)


```
kubectl get pod -n kube-system -l k8s-app=kube-dns -owide
```

coredns가 flannel에서 설정한 podCidr로 설정된 것을 확인할 수 있다. `10.244.0.0./16`

![](https://raw.githubusercontent.com/hyeonjae1122/hyeonjae1122.github.io/main/assets/20260124T074814206Z.png)


## 5.2 Metrics-server 설치

```
# metrics-server
helm repo add metrics-server https://kubernetes-sigs.github.io/metrics-server/
helm upgrade --install metrics-server metrics-server/metrics-server --set 'args[0]=--kubelet-insecure-tls' -n kube-system

# 확인
kubectl top node
kubectl top pod -A --sort-by='cpu'
kubectl top pod -A --sort-by='memory'
```


![](https://raw.githubusercontent.com/hyeonjae1122/hyeonjae1122.github.io/main/assets/20260124T075018528Z.png)



## 5.3 NFS subdir external provisioner 설치

- NFS subdir external provisioner 설치 : admin-lb 에 NFS Server(/srv/nfs/share) 설정 되어 있음
https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner

```bash

kubectl create ns nfs-provisioner
helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/
helm install nfs-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner -n nfs-provisioner \
    --set nfs.server=192.168.10.10 \
    --set nfs.path=/srv/nfs/share \
    --set storageClass.defaultClass=true

# 스토리지 클래스 확인
kubectl get sc

# 파드 확인
kubectl get pod -n nfs-provisioner -owide
```
## 5.4 kube-ops-view 설치

[https://hub.docker.com/r/abihf/kube-ops-view/tags](https://hub.docker.com/r/abihf/kube-ops-view/tags)

```
# kube-ops-view
helm repo add geek-cookbook https://geek-cookbook.github.io/charts/
# helm show values geek-cookbook/kube-ops-view

# macOS 사용자
helm install kube-ops-view geek-cookbook/kube-ops-view --version 1.2.2 \
  --set service.main.type=NodePort,service.main.ports.http.nodePort=30000 \
  --set env.TZ="Asia/Seoul" --namespace kube-system \
  --set image.repository="abihf/kube-ops-view" --set image.tag="latest"

# Windows 사용자
helm install kube-ops-view geek-cookbook/kube-ops-view --version 1.2.2 \
  --set service.main.type=NodePort,service.main.ports.http.nodePort=30000 \
  --set env.TZ="Asia/Seoul" --namespace kube-system 

# 설치 확인
kubectl get deploy,pod,svc,ep -n kube-system -l app.kubernetes.io/instance=kube-ops-view

# kube-ops-view 접속 URL 확인 (1.5 , 2 배율) : nodePor 이므로 IP는 all node 의 IP 가능!
open "http://192.168.10.101:30000/#scale=1.5"
open "http://192.168.10.101:30000/#scale=2"

```
![](https://raw.githubusercontent.com/hyeonjae1122/hyeonjae1122.github.io/main/assets/20260124T094713525Z.png)
## 5.5 kube-prometheus-stack 설치, 대시보드 추가

`vagrant ssh k8s-ctr1`

```bash
# repo 추가
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
```

`monitor-values.yaml` 생성

```bash
cat <<EOT > monitor-values.yaml
prometheus:
  prometheusSpec:
    scrapeInterval: "20s"
    evaluationInterval: "20s"
    storageSpec:
      volumeClaimTemplate:
        spec:
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 10Gi
    additionalScrapeConfigs:
      - job_name: 'haproxy-metrics'
        static_configs:
          - targets:
              - '192.168.10.10:8405'
    externalLabels:
      cluster: "myk8s-cluster"
  service:
    type: NodePort
    nodePort: 30001

grafana:
  defaultDashboardsTimezone: Asia/Seoul
  adminPassword: prom-operator
  service:
    type: NodePort
    nodePort: 30002

alertmanager:
  enabled: true
defaultRules:
  create: true

kubeProxy:
  enabled: false
prometheus-windows-exporter:
  prometheus:
    monitor:
      enabled: false
EOT
cat monitor-values.yaml
```


배포

```bash
helm install kube-prometheus-stack prometheus-community/kube-prometheus-stack --version 80.13.3 \
-f monitor-values.yaml --create-namespace --namespace monitoring
```


```bash
# 확인
helm list -n monitoring
kubectl get pod,svc,ingress,pvc -n monitoring
kubectl get prometheus,servicemonitors,alertmanagers -n monitoring
kubectl get crd | grep monitoring

# 각각 웹 접속 실행 : NodePort 접속
open http://192.168.10.101:30001 # prometheus
open http://192.168.10.101:30002 # grafana : 접속 계정 admin / prom-operator

# 프로메테우스 버전 확인
kubectl exec -it sts/prometheus-kube-prometheus-stack-prometheus -n monitoring -c prometheus -- prometheus --version
prometheus, version 3.9.1

# 그라파나 버전 확인
kubectl exec -it -n monitoring deploy/kube-prometheus-stack-grafana -- grafana --version
```


- 그라파나 대시보드 : 15661, 12693

```bash
# 대시보드 다운로드
curl -o 12693_rev12.json https://grafana.com/api/dashboards/12693/revisions/12/download
curl -o 15661_rev2.json https://grafana.com/api/dashboards/15661/revisions/2/download

# sed 명령어로 uid 일괄 변경 : 기본 데이터소스의 uid 'prometheus' 사용
sed -i -e 's/${DS_PROMETHEUS}/prometheus/g' 12693_rev12.json
sed -i -e 's/${DS__VICTORIAMETRICS-PROD-ALL}/prometheus/g' 15661_rev2.json

# my-dashboard 컨피그맵 생성 : Grafana 포드 내의 사이드카 컨테이너가 grafana_dashboard="1" 라벨 탐지!
kubectl create configmap my-dashboard --from-file=12693_rev12.json --from-file=15661_rev2.json -n monitoring
kubectl label configmap my-dashboard grafana_dashboard="1" -n monitoring

# 대시보드 경로에 추가 확인
kubectl exec -it -n monitoring deploy/kube-prometheus-stack-grafana -- ls -l /tmp/dashboards
-rw-r--r--    1 grafana  472         333790 Jan 22 06:27 12693_rev12.json
-rw-r--r--    1 grafana  472         198839 Jan 22 06:27 15661_rev2.json
...
```


- kcm, scheduler, etcd 메트릭 수집 될 수 있게 설정

`vagrant ssh k8s-ctr1`, `vagrant ssh k8s-ctr2`, `vagrant ssh k8s-ctr3`

```bash
# kube-controller-manager bind-address 127.0.0.1 => 0.0.0.0 변경
sed -i 's|--bind-address=127.0.0.1|--bind-address=0.0.0.0|g' /etc/kubernetes/manifests/kube-controller-manager.yaml
cat /etc/kubernetes/manifests/kube-controller-manager.yaml | grep bind-address
    - --bind-address=0.0.0.0

# kube-scheduler bind-address 127.0.0.1 => 0.0.0.0 변경
sed -i 's|--bind-address=127.0.0.1|--bind-address=0.0.0.0|g' /etc/kubernetes/manifests/kube-scheduler.yaml
cat /etc/kubernetes/manifests/kube-scheduler.yaml | grep bind-address
    - --bind-address=0.0.0.0

# etcd metrics-url(http) 127.0.0.1 에 192.168.10.100 추가
NODEIP=$(ip -4 addr show enp0s9 | grep -oP '(?<=inet\s)\d+(\.\d+){3}')
sed -i "s|--listen-metrics-urls=http://127.0.0.1:2381|--listen-metrics-urls=http://127.0.0.1:2381,http://$NODEIP:2381|g" /etc/kubernetes/manifests/etcd.yaml
cat /etc/kubernetes/manifests/etcd.yaml | grep listen-metrics-urls
    - --listen-metrics-urls=http://127.0.0.1:2381,http://192.168.10.11:2381
```


## 5.6 샘플 애플리케이션 배포, 반복 호출

샘플 애플리케이션 배포

```bash
cat << EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webpod
spec:
  replicas: 2
  selector:
    matchLabels:
      app: webpod
  template:
    metadata:
      labels:
        app: webpod
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - sample-app
            topologyKey: "kubernetes.io/hostname"
      containers:
      - name: webpod
        image: traefik/whoami
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: webpod
  labels:
    app: webpod
spec:
  selector:
    app: webpod
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
    nodePort: 30003
  type: NodePort
EOF
```


```
# 배포 확인
kubectl get deploy,svc,ep webpod -owide

# 반복 호출
SVCIP=$(kubectl get svc webpod -o jsonpath='{.spec.clusterIP}')
while true; do curl -s $SVCIP | grep Hostname; sleep 1; done

[admin-lb] # IP는 node 작업에 따라 변경
while true; do curl -s http://192.168.10.101:30003 | grep Hostname; sleep 1; done

```